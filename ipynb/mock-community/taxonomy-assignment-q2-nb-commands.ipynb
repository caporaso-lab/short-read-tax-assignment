{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Commands for `nb-extra` Mock Sweeps\n",
    "Performing broad sweeps for `nb-extra` required the use of a cluster.\n",
    "\n",
    "This script creates a temporary directory (`results_dir`) and generates two sets of shell commands.\n",
    "\n",
    "The commands rely on the Python modules in https://github.com/BenKaehler/q2-extra-classifier.\n",
    "\n",
    "Once the commands are generated in \"Classifier fitting scripts\", they can be run anywhere by copying the whole `results_dir` directory. The `results_dir` should then by syncronised back to its original location. The remainder of the script the copies the results into `tax-credit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from os.path import join\n",
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "import json\n",
    "from itertools import product\n",
    "\n",
    "from qiime2 import Artifact\n",
    "from qiime2.plugins import feature_classifier\n",
    "from q2_types.feature_data import DNAIterator\n",
    "from q2_feature_classifier.classifier import \\\n",
    "    spec_from_pipeline, pipeline_from_spec, _register_fitter\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "from tax_credit.framework_functions import \\\n",
    "    generate_per_method_biom_tables, move_results_to_repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Paths and Communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "project_dir = join('..', '..')\n",
    "analysis_name = 'mock-community'\n",
    "data_dir = join(project_dir, 'data', analysis_name)\n",
    "precomputed_dir = join(project_dir, 'data', 'precomputed-results', analysis_name)\n",
    "\n",
    "ref_db_dir = join(project_dir, 'data', 'ref_dbs')\n",
    "\n",
    "gg_db = join(ref_db_dir, 'gg_13_8_otus/99_otus.fasta')\n",
    "gg_tax = join(ref_db_dir, 'gg_13_8_otus/99_otu_taxonomy.txt')\n",
    "unite_db = join(ref_db_dir, 'unite_20.11.2016/sh_refs_qiime_ver7_99_20.11.2016_dev_clean.fasta')\n",
    "unite_tax = join(ref_db_dir, 'unite_20.11.2016/sh_taxonomy_qiime_ver7_99_20.11.2016_dev_clean.tsv')\n",
    "\n",
    "results_dir = join(project_dir, 'temp_results')\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "\n",
    "mock_dirs = ['mock-' + str(m) for m in (3, 12, 18, 22, 24, '26-ITS1', '26-ITS9')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Reference Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ref_dest = 'ref_dbs'\n",
    "refs = {m: join(ref_dest, '99_gg_seq.qza') for m in mock_dirs[:4]}\n",
    "refs.update({m: join(ref_dest, '99_unite_seq.qza') for m in mock_dirs[4:]})\n",
    "taxs = {m: join(ref_dest, '99_gg_tax.qza') for m in mock_dirs[:4]}\n",
    "taxs.update({m: join(ref_dest, '99_unite_tax.qza') for m in mock_dirs[4:]})\n",
    "\n",
    "if not os.path.exists(join(results_dir, ref_dest)):\n",
    "    os.makedirs(join(results_dir, ref_dest))\n",
    "ref = Artifact.import_data('FeatureData[Sequence]', gg_db)\n",
    "ref.save(join(results_dir, refs['mock-3']))\n",
    "with open(gg_tax) as fh:\n",
    "    reader = csv.reader(fh, delimiter='\\t')\n",
    "    data = {k:v for k, v in reader}\n",
    "data = Series(data, name='Taxon')\n",
    "data.index.name='Feature ID'\n",
    "tax = Artifact.import_data('FeatureData[Taxonomy]', data)\n",
    "tax.save(join(results_dir, taxs['mock-3']))\n",
    "ref = Artifact.import_data('FeatureData[Sequence]', unite_db)\n",
    "ref.save(join(results_dir, refs['mock-24']))\n",
    "with open(unite_tax) as fh:\n",
    "    reader = csv.reader(fh, delimiter='\\t')\n",
    "    data = {k:v for k, v in reader}\n",
    "data = Series(data, name='Taxon')\n",
    "data.index.name='Feature ID'\n",
    "tax = Artifact.import_data('FeatureData[Taxonomy]', data)\n",
    "tax.save(join(results_dir, taxs['mock-24']));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amplicon and Read Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mock-3 150 GTGCCAGCMGCCGCGGTAA GGACTACHVGGGTWTCTAAT\n",
      "mock-12 230 GTGCCAGCMGCCGCGGTAA GGACTACHVGGGTWTCTAAT\n",
      "mock-18 212 GTGCCAGCMGCCGCGGTAA GGACTACHVGGGTWTCTAAT\n",
      "mock-22 231 GTGCCAGCMGCCGCGGTAA GGACTACHVGGGTWTCTAAT\n",
      "mock-24 150 AACTTTYRRCAAYGGATCWCT AGCCTCCGCTTATTGATATGCTTAART\n",
      "mock-26-ITS1 290 CTTGGTCATTTAGAGGAAGTAA TCCTCCGCTTATTGATATGC\n",
      "mock-26-ITS9 290 GAACGCAGCRAANNGYGA TCCTCCGCTTATTGATATGC\n"
     ]
    }
   ],
   "source": [
    "def guess_read_length(seqs):\n",
    "    seqs = Artifact.load(seqs)\n",
    "    lengths = [len(s) for s in seqs.view(DNAIterator)]\n",
    "    lengths.sort()\n",
    "    return lengths[len(lengths)//2]\n",
    "\n",
    "def load_primers(primer_file):\n",
    "    with open(primer_file) as csvfile:\n",
    "        data = next(csv.DictReader(csvfile, delimiter='\\t'))\n",
    "        return data['LinkerPrimerSequence'], data['ReversePrimer']\n",
    "\n",
    "ref_dbs = {}\n",
    "for mock in mock_dirs:\n",
    "    mockdir = join(data_dir, mock)\n",
    "    repseqs = join(mockdir, 'rep_seqs.qza')\n",
    "    \n",
    "    if 'gg' in refs[mock]:\n",
    "        db_name = 'gg_13_8_otus_full'\n",
    "    else:\n",
    "        db_name = 'unite_20.11.2016_clean_full'\n",
    "    ref_dbs[mock] = [(db_name, refs[mock])]\n",
    "    \n",
    "    if not os.path.exists(repseqs):\n",
    "        continue\n",
    "    primerfile = join(mockdir, 'sample-metadata.tsv')\n",
    "    primers = list(load_primers(primerfile))\n",
    "    if primers[0] == 'CCGTGCCAGCMGCCGCGGTAA':\n",
    "        primers[0] = 'GTGCCAGCMGCCGCGGTAA'\n",
    "    elif 'I' in primers[0]:\n",
    "        primers[0] = primers[0].replace('I', 'N')\n",
    "    readlength = guess_read_length(repseqs)\n",
    "    print(os.path.basename(mockdir), str(readlength), *primers)\n",
    "    \n",
    "    ref = Artifact.load(join(results_dir, refs[mock]))\n",
    "    db_file = '_'.join(\n",
    "        [refs[mock].rsplit('.',1)[0], str(readlength)] + \n",
    "        list(primers)) + '.qza'\n",
    "    if 'gg' in refs[mock]:\n",
    "        db_name = 'gg_13_8_otus_read'\n",
    "    else:\n",
    "        db_name = 'unite_20.11.2016_clean_read'\n",
    "    ref_dbs[mock].append((db_name, db_file))\n",
    "    db_file = join(results_dir, db_file)\n",
    "    if not os.path.exists(db_file):\n",
    "        trimmed = feature_classifier.methods.extract_reads(\n",
    "                    sequences=ref, trunc_len=readlength,\n",
    "                    f_primer=primers[0], r_primer=primers[1]).reads\n",
    "        trimmed.save(db_file)\n",
    "    \n",
    "    db_file = '_'.join(\n",
    "        [refs[mock].rsplit('.',1)[0]] + \n",
    "        list(primers)) + '.qza'\n",
    "    if 'gg' in refs[mock]:\n",
    "        db_name = 'gg_13_8_otus_amplicon'\n",
    "    else:\n",
    "        db_name = 'unite_20.11.2016_clean_amplicon'\n",
    "    ref_dbs[mock].append((db_name, db_file))\n",
    "    db_file = join(results_dir, db_file)\n",
    "    if not os.path.exists(db_file):\n",
    "        trimmed = feature_classifier.methods.extract_reads(\n",
    "                    sequences=ref, f_primer=primers[0], r_primer=primers[1]).reads\n",
    "        trimmed.save(db_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Class Weights\n",
    "\n",
    "map taxonomies to taxonomy labels using `tax-credit/data/mock-community/mock-3/expected-taxonomy.tsv`, dickhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mock-3\n",
      "gg_13_8_otus_full\n",
      "gg_13_8_otus_read\n",
      "gg_13_8_otus_amplicon\n",
      "mock-12\n",
      "gg_13_8_otus_full\n",
      "gg_13_8_otus_read\n",
      "gg_13_8_otus_amplicon\n",
      "mock-18\n",
      "gg_13_8_otus_full\n",
      "gg_13_8_otus_read\n",
      "gg_13_8_otus_amplicon\n",
      "mock-22\n",
      "gg_13_8_otus_full\n",
      "gg_13_8_otus_read\n",
      "gg_13_8_otus_amplicon\n",
      "mock-24\n",
      "unite_20.11.2016_clean_full\n",
      "unite_20.11.2016_clean_read\n",
      "k__Fungi;p__Chytridiomycota;c__Chytridiomycetes;o__Spizellomycetales;f__Spizellomycetaceae;g__Spizellomyces;s__Spizellomyces_punctatus\n",
      "unite_20.11.2016_clean_amplicon\n",
      "k__Fungi;p__Chytridiomycota;c__Chytridiomycetes;o__Spizellomycetales;f__Spizellomycetaceae;g__Spizellomyces;s__Spizellomyces_punctatus\n",
      "mock-26-ITS1\n",
      "unite_20.11.2016_clean_full\n",
      "unite_20.11.2016_clean_read\n",
      "k__Fungi;p__Basidiomycota;c__Agaricomycetes;o__Cantharellales;f__Hydnaceae;g__Sistotrema;s__Sistotrema_brinkmannii\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetales_fam_Incertae_sedis;g__Debaryomyces;s__Debaryomyces_prosopidis\n",
      "k__Fungi;p__Basidiomycota;c__Agaricomycetes;o__Agaricales;f__Amanitaceae;g__Amanita;s__Amanita_lividopallescens\n",
      "k__Fungi;p__Ascomycota;c__Leotiomycetes;o__Helotiales;f__Helotiaceae;g__Hymenoscyphus;s__Hymenoscyphus_fraxineus\n",
      "k__Fungi;p__Ascomycota;c__Archaeorhizomycetes;o__Archaeorhizomycetales;f__Archaeorhizomycetaceae;g__Archaeorhizomyces;s__Archaeorhizomyces_finlayi\n",
      "k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Hypocreales;f__Nectriaceae;g__Fusarium;s__Fusarium_poae\n",
      "k__Fungi;p__Ascomycota;c__Pezizomycetes;o__Pezizales;f__Rhizinaceae;g__Rhizina;s__Rhizina_undulata\n",
      "k__Fungi;p__Basidiomycota;c__Agaricomycetes;o__Agaricales;f__Cortinariaceae;g__Cortinarius;s__Cortinarius_purpurascens\n",
      "unite_20.11.2016_clean_amplicon\n",
      "k__Fungi;p__Basidiomycota;c__Agaricomycetes;o__Cantharellales;f__Hydnaceae;g__Sistotrema;s__Sistotrema_brinkmannii\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetales_fam_Incertae_sedis;g__Debaryomyces;s__Debaryomyces_prosopidis\n",
      "k__Fungi;p__Basidiomycota;c__Agaricomycetes;o__Agaricales;f__Amanitaceae;g__Amanita;s__Amanita_lividopallescens\n",
      "k__Fungi;p__Ascomycota;c__Leotiomycetes;o__Helotiales;f__Helotiaceae;g__Hymenoscyphus;s__Hymenoscyphus_fraxineus\n",
      "k__Fungi;p__Ascomycota;c__Archaeorhizomycetes;o__Archaeorhizomycetales;f__Archaeorhizomycetaceae;g__Archaeorhizomyces;s__Archaeorhizomyces_finlayi\n",
      "k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Hypocreales;f__Nectriaceae;g__Fusarium;s__Fusarium_poae\n",
      "k__Fungi;p__Ascomycota;c__Pezizomycetes;o__Pezizales;f__Rhizinaceae;g__Rhizina;s__Rhizina_undulata\n",
      "k__Fungi;p__Basidiomycota;c__Agaricomycetes;o__Agaricales;f__Cortinariaceae;g__Cortinarius;s__Cortinarius_purpurascens\n",
      "mock-26-ITS9\n",
      "unite_20.11.2016_clean_full\n",
      "unite_20.11.2016_clean_read\n",
      "k__Fungi;p__Ascomycota;c__Leotiomycetes;o__Helotiales;f__Helotiaceae;g__Hymenoscyphus;s__Hymenoscyphus_fraxineus\n",
      "k__Fungi;p__Ascomycota;c__Archaeorhizomycetes;o__Archaeorhizomycetales;f__Archaeorhizomycetaceae;g__Archaeorhizomyces;s__Archaeorhizomyces_finlayi\n",
      "k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Hypocreales;f__Nectriaceae;g__Fusarium;s__Fusarium_poae\n",
      "k__Fungi;p__Basidiomycota;c__Agaricomycetes;o__Agaricales;f__Cortinariaceae;g__Cortinarius;s__Cortinarius_purpurascens\n",
      "unite_20.11.2016_clean_amplicon\n",
      "k__Fungi;p__Ascomycota;c__Leotiomycetes;o__Helotiales;f__Helotiaceae;g__Hymenoscyphus;s__Hymenoscyphus_fraxineus\n",
      "k__Fungi;p__Ascomycota;c__Archaeorhizomycetes;o__Archaeorhizomycetales;f__Archaeorhizomycetaceae;g__Archaeorhizomyces;s__Archaeorhizomyces_finlayi\n",
      "k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Hypocreales;f__Nectriaceae;g__Fusarium;s__Fusarium_poae\n",
      "k__Fungi;p__Basidiomycota;c__Agaricomycetes;o__Agaricales;f__Cortinariaceae;g__Cortinarius;s__Cortinarius_purpurascens\n"
     ]
    }
   ],
   "source": [
    "weights_dest = 'weights'\n",
    "if not os.path.exists(join(results_dir, weights_dest)):\n",
    "    os.makedirs(join(results_dir, weights_dest))\n",
    "\n",
    "priors_files = {}\n",
    "for mock in mock_dirs:\n",
    "    print(mock)\n",
    "    mockdir = join(data_dir, mock)\n",
    "    for db_name, db_file in ref_dbs[mock]:\n",
    "        print(db_name)\n",
    "        tax_weights = Artifact.load(join(results_dir, taxs[mock]))\n",
    "        seq_ids = Artifact.load(join(results_dir, db_file))\n",
    "        seq_ids = {s.metadata['id'] for s in seq_ids.view(DNAIterator)}\n",
    "        tax_weights = tax_weights.view(Series)\n",
    "        tax_weights = {tax_weights[sid]:0. for sid in tax_weights.index\n",
    "                       if sid in seq_ids}\n",
    "\n",
    "        weights = Artifact.load(join(mockdir, 'feature_table.qza'))\n",
    "        weights = weights.view(DataFrame)\n",
    "        if len(weights.index) > 1:\n",
    "            weights = {s:sum(weights.loc[s]) for s in weights.index}\n",
    "            total = sum(weights.values())\n",
    "            weights = {s:w/total for s, w in weights.items()}\n",
    "        else:\n",
    "            weights = {weights.index[0]: 1.}\n",
    "\n",
    "        et_path = join(precomputed_dir, mock)\n",
    "        if db_name.startswith('gg_13_8_otus'):\n",
    "            et_path = join(et_path, 'gg_13_8_otus')\n",
    "        else:\n",
    "            et_path = join(et_path, 'unite_20.11.2016_clean_fullITS')\n",
    "        et_path = join(et_path, 'expected', 'expected-taxonomy.tsv')\n",
    "        with open(et_path) as tf:\n",
    "            reader = csv.DictReader(tf, delimiter='\\t')\n",
    "            for row in reader:\n",
    "                tax = row['Taxonomy']\n",
    "                weight = sum(weights[s]*float(row[s]) for s in weights)\n",
    "                try:\n",
    "                    tax_weights[tax] += weight\n",
    "                except KeyError:\n",
    "                    species = {t for t in tax_weights if t.startswith(tax)}\n",
    "                    if len(species) == 0:\n",
    "                        print(tax)\n",
    "                    else:\n",
    "                        for s in species:\n",
    "                            tax_weights[s] += weight/len(species)\n",
    "\n",
    "        for tax in tax_weights:\n",
    "            if tax_weights[tax] < 1e-9:\n",
    "                tax_weights[tax] = 1e-9\n",
    "        total = sum(tax_weights.values())\n",
    "\n",
    "        weights = [tax_weights[t]/total for t in sorted(tax_weights)]\n",
    "        filename = mock + '-' + db_name + '-weights.json'\n",
    "        weights_file = join(weights_dest, filename)\n",
    "        priors_files[mock] = priors_files.get(mock, {})\n",
    "        priors_files[mock][db_name] = weights_file\n",
    "        weights_file = join(results_dir, weights_file)\n",
    "        with open(weights_file, 'w') as wf:\n",
    "            json.dump(weights, wf)\n",
    "        \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Parameter Sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_extra_sweep = \\\n",
    "    {'feat-ext--n-features': [1024, 8192, 65536],\n",
    "     'feat-ext--ngram-range': [[4,4], [8, 8], [16, 16], [4,16]],\n",
    "     'norm--norm': ['l1', 'l2', None],\n",
    "     'norm--use-idf': [True, False],\n",
    "     'classify--alpha': [0.001, 0.01, 0.1],\n",
    "     'classify--class-prior': ['uniform', 'prior']}\n",
    "        \n",
    "classifier_params = {'nb-extra': nb_extra_sweep}\n",
    "\n",
    "confidences = [0., 0.2, 0.4, 0.6, 0.8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier fitting scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_classifier_command(method, inputs, params, priors):\n",
    "    cmd = ['qiime feature-classifier fit-classifier-' + method]\n",
    "    cls = [method]\n",
    "    \n",
    "    for param in sorted(inputs):\n",
    "        value = inputs[param]\n",
    "        cmd.extend(['--i-' + param, value])\n",
    "        cls.append(os.path.basename(value).split('.')[0])\n",
    "    \n",
    "    for param in sorted(params):\n",
    "        value = params[param]\n",
    "        if value == 'prior':\n",
    "            cls.append(os.path.basename(priors).split('.')[0])\n",
    "        else:\n",
    "            cls.append(str(value).replace(' ',''))\n",
    "        \n",
    "        if type(value) is bool:\n",
    "            cmd.append('--p-' + ('' if value else 'no-') + param)\n",
    "            continue\n",
    "        \n",
    "        if 'class-prior' not in param:\n",
    "            value = json.dumps(value)\n",
    "            if value[0] != '\"' or value[-1] != '\"':\n",
    "                value = '\"' + value + '\"'\n",
    "            cmd.extend(['--p-' + param, value])\n",
    "            continue\n",
    "            \n",
    "        if value == 'uniform':\n",
    "            continue\n",
    "            \n",
    "        cmd.extend(['--p-' + param, '\"`cat ' + priors + '`\"'])\n",
    "    \n",
    "    cls = ':'.join(cls) + '.qza'\n",
    "    cls = os.path.sep + join('state', 'partition1', 'tmp', 'classifiers', cls)\n",
    "    \n",
    "    cmd.extend(['--o-classifier', '\"' + cls + '\"'])\n",
    "    cmd = ' '.join(cmd)\n",
    "    \n",
    "    return cls, cmd\n",
    "\n",
    "def get_classify_command(classifier, reads, params, \n",
    "                         confidence, directory, results_dir):\n",
    "    cmd = ['qiime feature-classifier classify-sklearn']\n",
    "    cmd.extend(['--i-classifier', '\"' + classifier + '\"'])\n",
    "    cmd.extend(['--i-reads', reads])\n",
    "    cmd.extend(['--p-confidence', str(confidence)])\n",
    "    \n",
    "    parameters = [str(params[p]).replace(' ', '') for p in sorted(params)]\n",
    "    parameters.append(str(confidence))\n",
    "    output_directory = join(directory, ':'.join(parameters))\n",
    "    if not os.path.exists(join(results_dir, output_directory)):\n",
    "        os.makedirs(join(results_dir, output_directory))\n",
    "    output = join(output_directory, 'rep_seqs_tax_assignments.qza')\n",
    "    cmd.extend(['--o-classification', '\"' + output + '\"'])\n",
    "    \n",
    "    return output, ' '.join(cmd)\n",
    "    \n",
    "def get_combinations(params):\n",
    "    params, values = zip(*params.items())\n",
    "    for combination in product(*values):\n",
    "        yield dict(zip(params, combination))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(join(results_dir, 'classifiers')):\n",
    "    os.makedirs(join(results_dir, 'classifiers'))\n",
    "\n",
    "classifier_commands = set()\n",
    "classify_commands = []\n",
    "classifiers = set()\n",
    "classifications = []\n",
    "for mock in mock_dirs:\n",
    "    reads = join('..', 'data', 'mock-community', mock, 'rep_seqs.qza')\n",
    "    mock_directory = join('classifications', mock)\n",
    "    inputs = {'reference-taxonomy': taxs[mock]}\n",
    "    for db_name, db_file in ref_dbs[mock]:\n",
    "        db_directory = join(mock_directory, db_name)\n",
    "        inputs['reference-reads'] = db_file\n",
    "        for method in classifier_params:\n",
    "            method_directory = join(db_directory, method)\n",
    "            for params in get_combinations(classifier_params[method]):\n",
    "                priors = priors_files[mock][db_name]\n",
    "                classifier, command = get_classifier_command(method, inputs, params, priors)\n",
    "                classifier_commands.add(command)\n",
    "                classifiers.add(classifier)\n",
    "                for confidence in confidences:\n",
    "                    classification, command = get_classify_command(\n",
    "                        classifier, reads, params, confidence,\n",
    "                        method_directory, results_dir)\n",
    "                    classifications.append(classification)\n",
    "                    classify_commands.append(command)\n",
    "                \n",
    "# write out the commands\n",
    "with open(join(results_dir, 'classifier_commands.sh'), 'w') as cmds:\n",
    "    for cmd in classifier_commands:\n",
    "        cmds.write(cmd + '\\n')\n",
    "with open(join(results_dir, 'classify_commands.sh'), 'w') as cmds:\n",
    "    for cmd in classify_commands:\n",
    "        cmds.write(cmd + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Additional files for tax-credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_classifications = []\n",
    "for classification in classifications:\n",
    "    full_classification = join(results_dir, classification)\n",
    "    output_dir = os.path.dirname(full_classification)\n",
    "\n",
    "    taxonomy_map_fp = join(output_dir, 'taxonomy.tsv')      \n",
    "    if not os.path.exists(taxonomy_map_fp):\n",
    "        try:\n",
    "            Artifact.load(full_classification).export_data(output_dir)\n",
    "        except ValueError:\n",
    "            bad_classifications.append(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742 bad classifications\n",
      "classifications/mock-26-ITS1/unite_20.11.2016_clean_full/nb-extra/0.001:uniform:65536:[4,4]:l1:True:0.0/rep_seqs_tax_assignments.qza\n",
      "classifications/mock-26-ITS1/unite_20.11.2016_clean_full/nb-extra/0.001:uniform:65536:[8,8]:l1:True:0.4/rep_seqs_tax_assignments.qza\n",
      "classifications/mock-26-ITS1/unite_20.11.2016_clean_full/nb-extra/0.001:uniform:65536:[4,4]:l2:True:0.0/rep_seqs_tax_assignments.qza\n",
      "classifications/mock-26-ITS1/unite_20.11.2016_clean_full/nb-extra/0.01:uniform:65536:[4,4]:l2:True:0.6/rep_seqs_tax_assignments.qza\n",
      "classifications/mock-26-ITS1/unite_20.11.2016_clean_full/nb-extra/0.1:uniform:65536:[4,4]:l2:True:0.0/rep_seqs_tax_assignments.qza\n",
      "classifications/mock-26-ITS1/unite_20.11.2016_clean_full/nb-extra/0.001:uniform:65536:[16,16]:l2:False:0.8/rep_seqs_tax_assignments.qza\n",
      "classifications/mock-26-ITS1/unite_20.11.2016_clean_full/nb-extra/0.01:uniform:65536:[4,4]:None:True:0.2/rep_seqs_tax_assignments.qza\n",
      "classifications/mock-26-ITS1/unite_20.11.2016_clean_full/nb-extra/0.01:uniform:65536:[4,4]:None:True:0.4/rep_seqs_tax_assignments.qza\n",
      "classifications/mock-26-ITS1/unite_20.11.2016_clean_full/nb-extra/0.001:uniform:65536:[8,8]:None:True:0.4/rep_seqs_tax_assignments.qza\n",
      "classifications/mock-26-ITS1/unite_20.11.2016_clean_full/nb-extra/0.001:uniform:65536:[16,16]:None:False:0.4/rep_seqs_tax_assignments.qza\n",
      "classifications/mock-26-ITS9/unite_20.11.2016_clean_full/nb-extra/0.001:uniform:65536:[8,8]:l1:True:0.0/rep_seqs_tax_assignments.qza\n",
      "classifications/mock-26-ITS9/unite_20.11.2016_clean_full/nb-extra/0.1:uniform:65536:[8,8]:l1:True:0.4/rep_seqs_tax_assignments.qza\n",
      "[{'classifications'}, {'mock-22', 'mock-3', 'mock-12', 'mock-18'}, {'gg_13_8_otus_full'}, {'nb-extra'}, {'0.001', '0.01', '0.1'}, {'uniform', 'prior'}, {'65536', '8192'}, {'[4,16]'}, {'l2', 'None', 'l1'}, {'True', 'False'}, {'0.6', '0.8', '0.2', '0.4', '0.0'}, {'rep_seqs_tax_assignments.qza'}]\n"
     ]
    }
   ],
   "source": [
    "print(len(bad_classifications), \"bad classifications\")\n",
    "bc_combinations = None\n",
    "for bc in bad_classifications:\n",
    "    if '[4,16]' not in bc:\n",
    "        print(bc)\n",
    "        continue\n",
    "    sbc = []\n",
    "    for tbc in bc.split(os.path.sep):\n",
    "        sbc.extend(tbc.split(':'))\n",
    "    if bc_combinations is None:\n",
    "        bc_combinations = [{tbc} for tbc in sbc]\n",
    "    else:\n",
    "        for tbc, bcc in zip(sbc, bc_combinations):\n",
    "            bcc.add(tbc)\n",
    "print(bc_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taxonomy_glob = join(results_dir, 'classifications', 'mock-*', '*', 'nb-extra', '*', 'taxonomy.tsv')\n",
    "generate_per_method_biom_tables(taxonomy_glob, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "precomputed_results_dir = join(project_dir, \"data\", \"precomputed-results\", analysis_name)\n",
    "method_dirs = glob.glob(join(results_dir, 'classifications', 'mock-*', '*', 'nb-extra', '*'))\n",
    "move_results_to_repository(method_dirs, precomputed_results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
